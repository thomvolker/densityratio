<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>densityratio ‚Ä¢ densityratio</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png">
<link rel="icon" type="‚Äùimage/svg+xml‚Äù" href="../favicon.svg">
<link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" sizes="any" href="../favicon.ico">
<link rel="manifest" href="../site.webmanifest">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="densityratio">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">densityratio</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.2.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item"><a class="nav-link" href="../articles/densityratio.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/covariate-shift.html">Covariate shift adjustment</a></li>
    <li><a class="dropdown-item" href="../articles/high-dim-testing.html">High dimensional two-sample testing</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/thomvolker/densityratio/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>densityratio</h1>
                        <h4 data-toc-skip class="author">Carlos Gonzalez
Poses &amp; Thom Benjamin Volker</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/thomvolker/densityratio/blob/main/vignettes/densityratio.Rmd" class="external-link"><code>vignettes/densityratio.Rmd</code></a></small>
      <div class="d-none name"><code>densityratio.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>Density ratio estimation is a powerful tool in many applications,
such as two-sample testing, classification, importance weighting and
evaluation of synthetic data utility. Each of these tasks can be
formulated as a problem of distribution comparison, where the goal is to
estimate where and how two distributions differ. The
<code>densityratio</code> package provides a collection of methods for
estimating the density ratio between two distributions. In this
vignette, we will provide an overview of the package and demonstrate how
to use it for distribution comparison tasks.</p>
</div>
<div class="section level2">
<h2 id="features">Features<a class="anchor" aria-label="anchor" href="#features"></a>
</h2>
<ul>
<li>
<strong>Fast</strong>: Computationally intensive code is executed in
<code>C++</code> using <code>Rcpp</code> and
<code>RcppArmadillo</code>.</li>
<li>
<strong>Automatic</strong>: Good default hyperparameters that can be
optimized in cross-validation (we do recommend understanding those
parameters before using <code>densityratio</code> in practice).</li>
<li>
<strong>Complete</strong>: Several density ratio estimation methods,
such as unconstrained least-squares importance fitting
(<code><a href="../reference/ulsif.html">ulsif()</a></code>), Kullback-Leibler importance estimation procedure
(<code><a href="../reference/kliep.html">kliep()</a></code>), ratio of estimated densities
(<code><a href="../reference/naive.html">naive()</a></code>), and extensions for high dimensional data
(least-squares heterodistributional subspace search, <code><a href="../reference/lhss.html">lhss()</a></code>
and spectral density ratio estimation, <code><a href="../reference/spectral.html">spectral()</a></code>).</li>
<li>
<strong>User-friendly</strong>: Simple user interface, default
<code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>, <code><a href="https://rdrr.io/r/base/print.html" class="external-link">print()</a></code> and <code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code>
functions for all density ratio estimation methods; built-in data sets
for quick testing.</li>
</ul>
</div>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<p>Currently, the package is available through GitHub and R universe.
You can download the package like so:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">'densityratio'</span>, repos <span class="op">=</span> <span class="st">'https://thomvolker.r-universe.dev'</span><span class="op">)</span></span></code></pre></div>
<p>Subsequently, we can easily load the package in the standard way.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://thomvolker.github.io/densityratio/">densityratio</a></span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="data">Data<a class="anchor" aria-label="anchor" href="#data"></a>
</h2>
<p>We apply the methods in the <code>densityratio</code> package to the
<code>kidiq</code> data that is introduced in <em>Data analysis using
regression and multilevel/hierarchical models</em> by Gelman and Hill
(2007) and also included in the package. This data set contains 434
observations measured on five variables:</p>
<ul>
<li>
<code>kid_score</code> Child‚Äôs IQ score (continuous)</li>
<li>
<code>mom_hs</code> Whether the mother obtained a high school degree
(binary)</li>
<li>
<code>mom_iq</code> Mother‚Äôs IQ score (continuous)</li>
<li>
<code>mom_work</code> Whether the mother worked in the first three
years of the child‚Äôs life (1: not in the first three years; 2: in the
second or third year; 3: parttime in the first year; 4: fulltime in the
first year)</li>
<li>
<code>mom_age</code> Mother‚Äôs age (continuous)</li>
</ul>
<p>For this example, we split the data according to the mother‚Äôs
education level, and evaluate how the distributions of the two groups
differ.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">high_school</span> <span class="op">&lt;-</span> <span class="va">kidiq</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/base/subset.html" class="external-link">subset</a></span><span class="op">(</span><span class="va">mom_hs</span> <span class="op">==</span> <span class="st">"yes"</span>, select <span class="op">=</span> <span class="op">-</span><span class="va">mom_hs</span><span class="op">)</span></span>
<span><span class="va">no_high_school</span> <span class="op">&lt;-</span> <span class="va">kidiq</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/base/subset.html" class="external-link">subset</a></span><span class="op">(</span><span class="va">mom_hs</span> <span class="op">==</span> <span class="st">"no"</span>, select <span class="op">=</span> <span class="op">-</span><span class="va">mom_hs</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="estimating-the-density-ratio">Estimating the density ratio<a class="anchor" aria-label="anchor" href="#estimating-the-density-ratio"></a>
</h2>
<p>Now we have the two groups we want to compare, we can evaluate how
the distributions of the two groups differ using the density ratio. That
is, we express differences between the two groups in terms of the ratio
of their densities:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ùê±</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><msub><mi>p</mi><mtext mathvariant="normal">hs</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>ùê±</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><msub><mi>p</mi><mtext mathvariant="normal">nhs</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>ùê±</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">
r(\mathbf{x}) = \frac{p_{\text{hs}}(\mathbf{x})}{p_{\text{nhs}}(\mathbf{x})},
</annotation></semantics></math> where the subscript refers to the
sample of observations from mothers with a high school degree (hs) or
without (nhs). The density ratio
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ùê±</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">r(\mathbf{x})</annotation></semantics></math>
directly shows how the two distributions differ at each point
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùê±</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math>
in the support of the data. If the density ratio is larger than 1, there
are relatively more observations in the high school group than in the no
high school group at that region, and vice versa if the density ratio is
smaller than one. Key to almost all estimation methods in the
<code>densityratio</code> package, is that we estimate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ùê±</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">r(\mathbf{x})</annotation></semantics></math>
directly as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>r</mi><mo accent="true">ÃÇ</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>ùê±</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat r(\mathbf{x})</annotation></semantics></math>,
without estimating the densities
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mtext mathvariant="normal">hs</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>ùê±</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p_{\text{hs}}(\mathbf{x})</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mtext mathvariant="normal">nhs</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>ùê±</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p_{\text{nhs}}(\mathbf{x})</annotation></semantics></math>
separately.</p>
<p>To estimate the density ratio, several estimation functions are
included.</p>
<ul>
<li>
<code><a href="../reference/kmm.html">kmm()</a></code>: Kernel Mean Matching estimates the density ratio
by matching the means of the two distributions in feature space (Huang
et al., 2007).</li>
<li>
<code><a href="../reference/kliep.html">kliep()</a></code>: Kullback-Leibler Importance Estimation
Procedure estimates the density ratio such that the Kullback-Leibler
divergence to the true density ratio function is minimized (Sugiyama et
al., 2007).</li>
<li>
<code><a href="../reference/ulsif.html">ulsif()</a></code>: Unconstrained Least-Squares Importance Fitting
provides an analytical solution for the density ratio by minimizing the
least-squares error (Pearson divergence) between the true density ratio
and the estimated density ratio (Kanamori et al., 2009).</li>
<li>
<code><a href="../reference/lhss.html">lhss()</a></code>: Least-Squares Heterodistributional Subspace
Search extends <code><a href="../reference/ulsif.html">ulsif()</a></code> to high-dimensional settings by
searching for the subspace where the numerator and denominator densities
are most different, and estimating the density ratio in that subspace
(Sugiyama et al., 2011).</li>
<li>
<code><a href="../reference/spectral.html">spectral()</a></code>: Spectral density ratio estimation is
another high-dimensional extension to classical density ratio estimation
approaches that performs dimension reduction through a spectral
decomposition in feature space (Izbicki et al., 2014).</li>
<li>
<code><a href="../reference/naive.html">naive()</a></code>: Naive density ratio estimation through
estimating the ratio of the two densities by separate density estimation
methods for the two groups.</li>
</ul>
<p>Which of these functions is most appropriate depends on the problem
at hand, but computational efficiency and relative robustness make
<code><a href="../reference/ulsif.html">ulsif()</a></code> a good starting point. For high-dimensional data,
<code><a href="../reference/spectral.html">spectral()</a></code> and <code><a href="../reference/lhss.html">lhss()</a></code> are presumably more
appropriate. All these estimation functions, except
<code><a href="../reference/naive.html">naive()</a></code>, are based on the same principle of minimizing a
divergence between the true density ratio and the estimated density
ratio, but they differ in the divergence that is minimized and the
constraints that are imposed on the estimated density ratio. These
estimation functions are kernel-based and thereby non-parametric.
Specifically, we use the following linear model for the density ratio:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>r</mi><mo accent="true">ÃÇ</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>ùê±</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>K</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ùê±</mi><mo>,</mo><mrow><mi>ùê±</mi><mi mathvariant="bold">‚Ä≤</mi></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mover><mi>ùõâ</mi><mo accent="true">ÃÇ</mo></mover><mo>,</mo></mrow><annotation encoding="application/x-tex">
\hat{r}(\mathbf{x}) = K(\mathbf{x}, \mathbf{x'})\hat{\mathbf{\theta}},
</annotation></semantics></math> with the kernel function defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ùê±</mi><mo>,</mo><mrow><mi>ùê±</mi><mi mathvariant="bold">‚Ä≤</mi></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>exp</mo><mo minsize="3.0" maxsize="3.0" stretchy="false" form="prefix">(</mo><mo>‚àí</mo><mfrac><mrow><mrow><mo stretchy="true" form="prefix">|</mo><mo stretchy="true" form="postfix">|</mo></mrow><mi>ùê±</mi><mo>‚àí</mo><mrow><mi>ùê±</mi><mi mathvariant="bold">‚Ä≤</mi></mrow><mrow><mo stretchy="true" form="prefix">|</mo><mo stretchy="true" form="postfix">|</mo></mrow></mrow><mrow><mn>2</mn><msup><mi>œÉ</mi><mn>2</mn></msup></mrow></mfrac><mo minsize="3.0" maxsize="3.0" stretchy="false" form="postfix">)</mo><mo>,</mo></mrow><annotation encoding="application/x-tex">
K(\mathbf{x}, \mathbf{x'}) = \exp \Bigg(-\frac{||\mathbf{x}-\mathbf{x'}||}{2\sigma^2} \Bigg),
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œÉ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>
is the bandwidth of the kernel and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>ùõâ</mi><mo accent="true">ÃÇ</mo></mover><annotation encoding="application/x-tex">\hat{\mathbf{\theta}}</annotation></semantics></math>
is a vector of parameters that we estimate from the data. While the
model is linear in the parameters, the estimate of the density ratio is
non-linear through the use of the kernel function. Currently, we use
Gaussian kernels for all methods (but we plan to allow for different
kernels in a future version of the package), which uses the following
transformation of the data. Note that the functions <code><a href="../reference/lhss.html">lhss()</a></code>
and <code><a href="../reference/spectral.html">spectral()</a></code> use a slightly more involved transformation
of the data to perform dimension reduction. All estimation functions
typically require extensive tuning of hyperparameters, such as the
bandwidth of the kernel and potentially a regularization parameter. This
tuning is taken care of through in-built cross-validation, such that
very little user-specification is required. Below, we illustrate the
package by estimating the density ratio using <code><a href="../reference/ulsif.html">ulsif()</a></code>.</p>
<div class="section level3">
<h3 id="input-data">Input data<a class="anchor" aria-label="anchor" href="#input-data"></a>
</h3>
<p>The density ratio package expects two data.frames as input, one for
the numerator observations, and one for the denominator observations.
These data sets must have the same variables: if this assumption is
violated, the function returns an error.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span> <span class="co"># for reproducibility</span></span>
<span><span class="va">dr</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ulsif.html">ulsif</a></span><span class="op">(</span><span class="va">high_school</span>, <span class="va">no_high_school</span><span class="op">)</span></span></code></pre></div>
<p>All estimation functions return an object of their respective class
(‚Äúulsif‚Äù, ‚Äúkmm‚Äù, ‚Äúkliep‚Äù, and so on). Each method has a separate
<code>S3</code> class, because of differences in models, hyperparameters
and important information that is included in the object. Still, all
classes have a common interface with <code><a href="https://rdrr.io/r/base/print.html" class="external-link">print()</a></code>,
<code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code> and <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code> methods. Printing the
output of the object <code>dr</code> provides some important information
of the fitted model:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">dr</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; ulsif(df_numerator = high_school, df_denominator = no_high_school)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Kernel Information:</span></span>
<span><span class="co">#&gt;   Kernel type: Gaussian with L2 norm distances</span></span>
<span><span class="co">#&gt;   Number of kernels: 200</span></span>
<span><span class="co">#&gt;   sigma: num [1:10] 0.913 1.216 1.393 1.551 1.71 ...</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Regularization parameter (lambda): num [1:20] 1000 483.3 233.6 112.9 54.6 ...</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Optimal sigma (loocv): 1.216132</span></span>
<span><span class="co">#&gt; Optimal lambda (loocv): 0.3359818</span></span>
<span><span class="co">#&gt; Optimal kernel weights (loocv): num [1:201] 0.3256 -0.054 0.0098 0.1639 0.2047 ...</span></span>
<span><span class="co">#&gt; </span></span></code></pre></div>
<p>The output shows the number of kernels, the candidate values for the
bandwidth parameter <code>sigma</code>, the candidates for the
regularization parameter <code>lambda</code> and the optimal values of
these parameters, including the weights that can be used to compute the
density ratio for new samples. It is also possible to use different
candidate parameters for the bandwidth and regularization parameter,
which can be done in different ways.</p>
</div>
<div class="section level3">
<h3 id="hyperparameter-selection">Hyperparameter selection<a class="anchor" aria-label="anchor" href="#hyperparameter-selection"></a>
</h3>
<div class="section level4">
<h4 id="specifying-the-inducing-points-in-the-kernel">Specifying the inducing points in the kernel<a class="anchor" aria-label="anchor" href="#specifying-the-inducing-points-in-the-kernel"></a>
</h4>
<p>Kernel-based algorithms are computationally expensive: in the
original form, the distance from observations to every other observation
is used to calculate the density ratio. This means that the
computational cost scales cubically with the number of observations in
the data. To alleviate this issue, we make use of inducing points, which
are a subset of the observations that are used to calculate the distance
matrix. The idea hereof is that we do not use all observations, because
most are relatively close to each other and share similar information.
By default, the estimation functions use 200 inducing points, randomly
sampled from the numerator and denominator data, but this number can be
altered by changing the <code>ncenters</code> parameter. It is also
possible to set the inducing points manually, by providing a
<code>data.frame</code> with the same variables as the input data,
containing the inducing points in the rows, to the <code>centers</code>
argument. If the inducing points are set manually, the
<code>ncenters</code> parameter is ignored.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">dr</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ulsif.html">ulsif</a></span><span class="op">(</span><span class="va">high_school</span>, <span class="va">no_high_school</span>, ncenters <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span><span class="va">dr</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ulsif.html">ulsif</a></span><span class="op">(</span><span class="va">high_school</span>, <span class="va">no_high_school</span>, centers <span class="op">=</span> <span class="va">high_school</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="bandwidth-and-regularization-parameter">Bandwidth and regularization parameter<a class="anchor" aria-label="anchor" href="#bandwidth-and-regularization-parameter"></a>
</h4>
<p>The bandwidth of the kernel and the regularization parameter are
tuned automatically using leave-one-out-cross-validation, which can be
calculated analytically. By default, the candidate bandwidth values are
chosen by taking quantiles of the interpoint distance matrix
corresponding to 10 equally spaced probabilities between 0.05 and 0.95.
Changing the <code>nsigma</code> parameter results in a different number
of quantiles used. The default candidate values for the regularization
parameter <code>lambda</code> are set to
<code>10^{seq(-3, 3, length.out = 20)}</code>. By changing
<code>nlambda</code>, the number of candidate values in this sequence
can be altered, to estimate the optimal hyperparameters with higher
precision.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">dr</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ulsif.html">ulsif</a></span><span class="op">(</span><span class="va">high_school</span>, <span class="va">no_high_school</span>, nsigma <span class="op">=</span> <span class="fl">20</span>, nlambda <span class="op">=</span> <span class="fl">50</span><span class="op">)</span></span>
<span><span class="va">dr</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; ulsif(df_numerator = high_school, df_denominator = no_high_school,     nsigma = 20, nlambda = 50)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Kernel Information:</span></span>
<span><span class="co">#&gt;   Kernel type: Gaussian with L2 norm distances</span></span>
<span><span class="co">#&gt;   Number of kernels: 200</span></span>
<span><span class="co">#&gt;   sigma: num [1:20] 0.864 1.082 1.181 1.267 1.346 ...</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Regularization parameter (lambda): num [1:50] 1000 754 569 429 324 ...</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Optimal sigma (loocv): 1.345547</span></span>
<span><span class="co">#&gt; Optimal lambda (loocv): 0.4941713</span></span>
<span><span class="co">#&gt; Optimal kernel weights (loocv): num [1:201] 0.2213 -0.0349 -0.0107 0.0222 -0.0175 ...</span></span>
<span><span class="co">#&gt; </span></span></code></pre></div>
<p>It is also possible to specify the candidate parameter values
manually. For the bandwidth, this can be done by specifying the
probabilities used in calculation of the quantiles, using
<code>sigma_quantile</code>, or by specifying the bandwidth values
directly, using <code>sigma</code>. For the regularization parameter,
this can be done by specifying the candidate values directly using
<code>lambda</code>.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">dr</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ulsif.html">ulsif</a></span><span class="op">(</span></span>
<span>  <span class="va">high_school</span>,</span>
<span>  <span class="va">no_high_school</span>,</span>
<span>  sigma <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1.1</span>, <span class="fl">1.2</span><span class="op">)</span>,</span>
<span>  lambda <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">1</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Other estimation functions have similar arguments, although only
<code><a href="../reference/ulsif.html">ulsif()</a></code> and <code><a href="../reference/lhss.html">lhss()</a></code> accept a regularization
parameter.</p>
<div class="section level5">
<h5 id="intermezzo-bandwidth-regularization-parameter">Intermezzo: Bandwidth &amp; regularization parameter<a class="anchor" aria-label="anchor" href="#intermezzo-bandwidth-regularization-parameter"></a>
</h5>
<blockquote>
<p><strong>Bandwidth parameter:</strong> The bandwidth parameter
controls how flexible the density ratio model is. Smaller values place
relatively more weight on observations that are very close to the
observation at hand, whereas larger values also borrow information from
observations that are further away in feature space. A smaller smaller
bandwidth allows to model sudden shifts in the density ratio, but might
also be prone to overfitting, while a larger bandwidth results in a
smoother estimate. Below, we illustrate this by plotting the estimated
density ratio for a single variable, <code>kid_score</code>, estimated
with both a large and a small bandwidth parameter. While both estimates
show that the density ratio is larger for larger values of
<code>kid_score</code>, the estimate with the smaller bandwidth
parameter is much more flexible and less smooth.</p>
<div class="float">
<img src="densityratio-bandwidth-note-1.png" alt="Density ratio estimates for a large and small bandwidth parameter."><div class="figcaption">Density ratio estimates for a large and small
bandwidth parameter.</div>
</div>
</blockquote>
<blockquote>
<p><strong>Regularization parameter</strong>: The regularization
parameter controls the amount of regularization applied to the model. A
larger value of the regularization parameter results in a smoother
estimate, while a smaller value allows for more flexibility in the
model. This is shown below, where we plot the estimated density ratio
for <code>kid_score</code> estimated with both a large and a small
regularization parameter. Again, the estimate with the smaller
regularization parameter is much more flexible and less smooth. Note,
here, that regularization shrinks the kernel weights towards zero, such
that the estimated density ratio tends to zero (and not to one, as one
might expect). We plan to investigate this further in the future.</p>
<div class="float">
<img src="densityratio-regularization-note-1.png" alt="Density ratio estimates for a large and small regularization parameter."><div class="figcaption">Density ratio estimates for a large and small
regularization parameter.</div>
</div>
</blockquote>
</div>
</div>
</div>
<div class="section level3">
<h3 id="scaling-of-the-data">Scaling of the data<a class="anchor" aria-label="anchor" href="#scaling-of-the-data"></a>
</h3>
<p>By default, all data is scaled before estimating the density ratio.
Because the density ratio is distance-based, variables with larger
variance may contribute more to the distance, and are thus implicitly
deemed more important in the model, than variables with smaller
variance. By default, the data is scaled such that continuous variables
in the numerator data has mean 0 and variance 1, and categorical
variables (factors and character strings) are turned into dummy
variables (i.e., one-hot encoded). The denominator data is scaled
accordingly, using the means and variances of the numerator data. If
desired, this can be changed by setting the <code>scale</code> argument
to <code>scale = "denominator"</code> to use the denominator means and
variances, or to <code>scale = NULL</code>, to apply no scaling at all.
Additionally, an intercept is added to the model matrix by default. This
can be turned off by setting <code>intercept = FALSE</code>.</p>
</div>
<div class="section level3">
<h3 id="computational-efficiency">Computational efficiency<a class="anchor" aria-label="anchor" href="#computational-efficiency"></a>
</h3>
<p>Parallel computation is natively supported by most functions in the
density ratio package (current exceptions are <code><a href="../reference/lhss.html">lhss()</a></code> and
<code><a href="../reference/kliep.html">kliep()</a></code>, for which storage requirements can defeat speedup
due to parallelization, we aim to solve this in a future version).
Parallel computation can be enabled simply by setting
<code>parallel = TRUE</code>. By default, the number of available
threads minus one is used, but this can be changed by setting the
<code>nthreads</code> argument. Note that the number specified in
<code>nthreads</code> is not only limited by the number of cores on the
machine, but also by the number of hyperparameters than can be estimated
in parallel (such as <code>nlambda</code>).</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">dr</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ulsif.html">ulsif</a></span><span class="op">(</span></span>
<span>  <span class="va">high_school</span>,</span>
<span>  <span class="va">no_high_school</span>,</span>
<span>  parallel <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  nthreads <span class="op">=</span> <span class="fl">10</span></span>
<span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="other-estimation-functions">Other estimation functions<a class="anchor" aria-label="anchor" href="#other-estimation-functions"></a>
</h3>
<p>In this section, we briefly skim over the other estimation functions
available in the package. These functions are all built on the same
principles, but use different algorithms with different hyperparameters.
These hyperparameters are still tuned automatically, but use
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-fold
cross-validation instead of leave-one-out cross-validation. Although
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
can be set to equal the minimum number of observations in the numerator
or denominator data, which yields leave-one-out cross-validation, this
might significantly affect the computational cost, because no analytical
solution to the loss is available.</p>
<div class="section level4">
<h4 id="kmm">
<code>kmm()</code><a class="anchor" aria-label="anchor" href="#kmm"></a>
</h4>
<p>The kernel mean matching algorithm is quite similar to the
<code><a href="../reference/ulsif.html">ulsif()</a></code> algorithm, but starts from a different perspective:
we estimate the density ratio such that reweighing the denominator
samples with the density ratio results in a distribution that is as
similar as possible to the numerator distribution in terms of the L2
norm. This approach does not require tuning of a regularization
parameter, but does require tuning of the bandwidth parameter (the same
defaults as for <code><a href="../reference/ulsif.html">ulsif()</a></code> apply). By default,
<code><a href="../reference/kmm.html">kmm()</a></code> uses unconstrained optimization to estimate the
density ratio parameters <code>constrained = FALSE</code>, but this can
be changed to <code>constrained = TRUE</code> to use constrained
optimization. The unconstrained optimization is more efficient, but
might yield negative estimates for the density ratio. Additionally,
cross-validation can be disabled by setting <code>cv = FALSE</code>, and
the number of cross-validation folds can be altered by changing
<code>nfold</code> parameter.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">dr_kmm</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/kmm.html">kmm</a></span><span class="op">(</span></span>
<span>  <span class="va">high_school</span>,</span>
<span>  <span class="va">no_high_school</span>,</span>
<span>  constrained <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  nfold <span class="op">=</span> <span class="fl">10</span>,</span>
<span>  parallel <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">dr_kmm</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; kmm(df_numerator = high_school, df_denominator = no_high_school,     constrained = TRUE, nfold = 10, parallel = TRUE)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Kernel Information:</span></span>
<span><span class="co">#&gt;   Kernel type: Gaussian with L2 norm distances</span></span>
<span><span class="co">#&gt;   Number of kernels: 200</span></span>
<span><span class="co">#&gt;   sigma: num [1:10] 0.989 1.252 1.444 1.606 1.772 ...</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Optimal sigma (10-fold cv): 1.606</span></span>
<span><span class="co">#&gt; Optimal kernel weights (10-fold cv):  num [1:200, 1] 3.16e-04 9.80e-05 -6.06e-05 5.70e-06 -2.26e-04 ...</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Optimization parameters:</span></span>
<span><span class="co">#&gt;   Optimization method:  Constrained</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="kliep">
<code>kliep()</code><a class="anchor" aria-label="anchor" href="#kliep"></a>
</h4>
<p>The Kullback-Leibler importance fitting procedure optimizes a
somewhat different loss compared to <code><a href="../reference/ulsif.html">ulsif()</a></code> (the
Kullback-Leibler loss, rather than the Pearson loss), and does not
include a regularization parameter. Whereas <code><a href="../reference/ulsif.html">ulsif()</a></code> can be
estimated analytically, <code><a href="../reference/kliep.html">kliep()</a></code> uses a gradient descent
algorithm to estimate the parameters. The <code><a href="../reference/kliep.html">kliep()</a></code> function
has some additional arguments for the optimization routine, such as the
maximum number of iterations <code>maxit</code> and the learning rate
<code>epsilon</code>. Again, hyperparameter values and cross-validation
scheme can be altered as described above.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">dr_kliep</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/kliep.html">kliep</a></span><span class="op">(</span></span>
<span>  <span class="va">high_school</span>,</span>
<span>  <span class="va">no_high_school</span>,</span>
<span>  nsigma <span class="op">=</span> <span class="fl">20</span>,</span>
<span>  maxit <span class="op">=</span> <span class="fl">10000</span>,</span>
<span>  epsilon <span class="op">=</span> <span class="fl">0.0001</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">dr_kliep</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; kliep(df_numerator = high_school, df_denominator = no_high_school,     nsigma = 20, epsilon = 1e-04, maxit = 10000)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Kernel Information:</span></span>
<span><span class="co">#&gt;   Kernel type: Gaussian with L2 norm distances</span></span>
<span><span class="co">#&gt;   Number of kernels: 200</span></span>
<span><span class="co">#&gt;   sigma: num [1:20] 0.914 1.11 1.207 1.298 1.377 ...</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Optimal sigma (5-fold cv): 0.9142</span></span>
<span><span class="co">#&gt; Optimal kernel weights (5-fold cv):  num [1:200, 1] 0.278 0.0659 0.0434 0.1052 0.2286 ...</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Optimization parameters:</span></span>
<span><span class="co">#&gt;   Learning rate (epsilon): 1e-04</span></span>
<span><span class="co">#&gt;   Maximum number of iterations:  10000</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="lhss">
<code>lhss()</code><a class="anchor" aria-label="anchor" href="#lhss"></a>
</h4>
<p>The <code><a href="../reference/lhss.html">lhss()</a></code> function implements the least-squares
heterodistributional subspace search, which is a high-dimensional
extension to <code><a href="../reference/ulsif.html">ulsif()</a></code>. Rather than applying
<code><a href="../reference/ulsif.html">ulsif()</a></code> to the original input data, <code><a href="../reference/lhss.html">lhss()</a></code>
first finds a lower-dimensional subspace where the numerator and
denominator samples are maximally different, and then applies
<code><a href="../reference/ulsif.html">ulsif()</a></code> to the data projected to this subspace. Note that
this subspace is a linear projection of the original data. This method
works well if such a subspace indeed exists, but might not be optimal if
such a linear projection cannot capture the differences between the two
distributions. Parameters for <code><a href="../reference/lhss.html">lhss()</a></code> are similar to those
for <code><a href="../reference/ulsif.html">ulsif()</a></code>, although the bandwidth values are by default
obtained after the projection (that is, <code>nsigma</code> and
<code>sigma_quantile</code> are applied to the projected data, but
<code>sigma</code> is considered as is). One parameter that is specific
to <code><a href="../reference/lhss.html">lhss()</a></code> concerns the dimensionality of the subspace,
which can be set by changing <code>m</code>, which defaults to the
square root of the number of features in the data. Because the algorithm
is quite expensive computationally, no optimization with respect to the
subspace is carried out. Different choices can be considered by running
the algorithm with different values of <code>m</code>, but this requires
multiple calls to the <code>lhss</code> function. Currently, no parallel
computation is supported for <code><a href="../reference/lhss.html">lhss()</a></code>, but this is planned
for a future version, together with native support for optimizing the
dimension of the subspace. Furthermore, the function uses a gradient
descent algorithm to find the subspace: the number of iterations used
defaults to <code>maxit = 200</code> but can be adapted by the user.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">dr_lhss</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/lhss.html">lhss</a></span><span class="op">(</span></span>
<span>  <span class="va">high_school</span>,</span>
<span>  <span class="va">no_high_school</span>,</span>
<span>  m <span class="op">=</span> <span class="fl">1</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">dr_lhss</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lhss(df_numerator = high_school, df_denominator = no_high_school,     m = 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Kernel Information:</span></span>
<span><span class="co">#&gt;   Kernel type: Gaussian with L2 norm distances</span></span>
<span><span class="co">#&gt;   Number of kernels: 200</span></span>
<span><span class="co">#&gt;   sigma: num [1:10, 1:10] 0.00514 0.04845 0.13661 0.27564 0.4775 ...</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Regularization parameter (lambda): num [1:10] 1000 215.44 46.42 10 2.15 ...</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Subspace dimension (m): 1</span></span>
<span><span class="co">#&gt; Optimal sigma: 0.1366127</span></span>
<span><span class="co">#&gt; Optimal lambda: 0.1</span></span>
<span><span class="co">#&gt; Optimal kernel weights (loocv): num [1:201] 1.3467 0 0.1011 0.0874 0 ...</span></span>
<span><span class="co">#&gt; </span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="spectral">
<code>spectral()</code><a class="anchor" aria-label="anchor" href="#spectral"></a>
</h4>
<p>The spectral() method offers an alternative way to estimate the
density ratio in high-dimensional settings. Instead of reducing the
dimensionality before applying a kernel, it first applies a kernel to
the denominator data and performs an eigen decomposition to find a
low-dimensional representation. As computing this eigen decomposition is
computationally expensive, it is also possible to use only a subset of
the denominator data to compute this eigen decomposition (by setting the
parameter <code>ncenters</code>, similarly to the other functions). The
density ratio is then estimated in this reduced space by matching the
numerator data to the denominator data. The goal is to find weights such
that, when applied to the denominator data in kernel space, the
resulting distribution closely resembles the numerator distribution.
Similarly to <code><a href="../reference/lhss.html">lhss()</a></code>, the <code><a href="../reference/spectral.html">spectral()</a></code> function
has a parameter <code>m</code> that determines the dimensionality of the
subspace, which defaults to an evenly-spaced sequence of length 50
between 1 and the number of centers in the denominator data (or the size
of the subset used). Again, cross-validation is used to find the optimal
value of <code>m</code> and the optimal bandwidth parameter. The number
of folds can be altered using the <code>nfold</code> parameter, and
cross-validation can be disabled by setting <code>cv = FALSE</code>.
Also, parallel computation is supported, and can be enabled by setting
<code>parallel = TRUE</code>, potentially with a different number of
threads using the <code>nthreads</code> parameter.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">dr_spectral</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/spectral.html">spectral</a></span><span class="op">(</span></span>
<span>  <span class="va">high_school</span>,</span>
<span>  <span class="va">no_high_school</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">dr_spectral</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; spectral(df_numerator = high_school, df_denominator = no_high_school)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Kernel Information:</span></span>
<span><span class="co">#&gt;   Kernel type: Gaussian with L2 norm distances</span></span>
<span><span class="co">#&gt;   Number of kernels: 93</span></span>
<span><span class="co">#&gt;   sigma: num [1:10] 1.01 1.28 1.47 1.65 1.82 ...</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Subspace dimension (J): num [1:50] 1 2 4 6 7 9 11 12 14 16 ...</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Optimal sigma: 3.215855</span></span>
<span><span class="co">#&gt; Optimal subspace: 16</span></span>
<span><span class="co">#&gt; Optimal kernel weights (cv): num [1:16] 0.952 -0.461 -0.755 -0.559 -0.342 ...</span></span>
<span><span class="co">#&gt; </span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="naive">
<code>naive()</code><a class="anchor" aria-label="anchor" href="#naive"></a>
</h4>
<p>Naive density ratio estimation estimates the density ratio by
estimating the numerator and denominator densities separately. We use a
singular value decomposition to account for relationships between the
features, estimated on the numerator data and project the denominator
data onto the same subspace. As such, dimension reduction is readily
implemented. Subsequently, we make use of the naive Bayes assumption,
implying independence of the features, to estimate the density ratio on
the latent space. Hence, the <code><a href="../reference/naive.html">naive()</a></code> function has a
parameter <code>m</code> that determines the dimensionality of the
subspace, which defaults to the number of variables in the data (no
dimension reduction). Additionally parameters are inherited from the
<code><a href="https://rdrr.io/r/stats/density.html" class="external-link">density()</a></code> function, which is used to estimate the numerator
and denominator densities. Note that we do not advice this method, it is
merely included as a benchmark.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">dr_naive</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/naive.html">naive</a></span><span class="op">(</span></span>
<span>  <span class="va">high_school</span>,</span>
<span>  <span class="va">no_high_school</span>,</span>
<span>  m <span class="op">=</span> <span class="fl">2</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">dr_naive</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; naive(df_numerator = high_school, df_denominator = no_high_school,     m = 2)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Naive density ratio</span></span>
<span><span class="co">#&gt;   Number of variables: 4</span></span>
<span><span class="co">#&gt;   Number of numerator samples: 341</span></span>
<span><span class="co">#&gt;   Number of denominator samples: 93</span></span>
<span><span class="co">#&gt;   Numerator density: num [1:341] 3.82 2.43 5.32 2.09 4.13 ...</span></span>
<span><span class="co">#&gt;   Denominator density: num [1:93] 0.459 0.417 0.629 1.823 0.434 ...</span></span>
<span><span class="co">#&gt; </span></span></code></pre></div>
</div>
</div>
</div>
<div class="section level2">
<h2 id="summarizing-the-density-ratio-objects">Summarizing the density ratio objects<a class="anchor" aria-label="anchor" href="#summarizing-the-density-ratio-objects"></a>
</h2>
<p>The <code>densityratio</code> package contains several functions to
help interpreting the output of each estimation method. The
<code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code> function provides a brief summary of the
estimated object, including the optimal hyperparameter values, the
number of inducing points and the divergence between the numerator and
denominator samples.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">dr</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; ulsif(df_numerator = high_school, df_denominator = no_high_school,     parallel = TRUE, nthreads = 10)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Kernel Information:</span></span>
<span><span class="co">#&gt;   Kernel type: Gaussian with L2 norm distances</span></span>
<span><span class="co">#&gt;   Number of kernels: 200</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Optimal sigma: 1.387557</span></span>
<span><span class="co">#&gt; Optimal lambda: 0.6951928</span></span>
<span><span class="co">#&gt; Optimal kernel weights: num [1:201] 0.16916 0.00301 0.05082 -0.00791 0.03594 ...</span></span>
<span><span class="co">#&gt;  </span></span>
<span><span class="co">#&gt; Pearson divergence between P(nu) and P(de): 0.4149</span></span>
<span><span class="co">#&gt; For a two-sample homogeneity test, use 'summary(x, test = TRUE)'.</span></span></code></pre></div>
<p>Which divergence measure is reported depends on the method used. The
<code><a href="../reference/ulsif.html">ulsif()</a></code>, <code><a href="../reference/kmm.html">kmm()</a></code>, <code><a href="../reference/lhss.html">lhss()</a></code> and
<code><a href="../reference/spectral.html">spectral()</a></code> methods report the Pearson divergence, while the
<code><a href="../reference/kliep.html">kliep()</a></code> method reports the Kullback-Leibler divergence. The
<code><a href="../reference/naive.html">naive()</a></code> method reports the squared difference between the
average log density ratio of the numerator samples and the average log
density ratio of the denominator samples. As these divergences are hard
to interpret in isolation and provide relatively little information on
where the two distributions differ, we recommend to use the plotting
functionality that is described in the next section. Alternatively, if
the goal is to evaluate the distributions more formally, the divergence
measures can be used for two-sample homogeneity testing.</p>
<div class="section level4">
<h4 id="two-sample-homogeneity-testing">Two-sample homogeneity testing<a class="anchor" aria-label="anchor" href="#two-sample-homogeneity-testing"></a>
</h4>
<p>Two sampling testing functionality is readily implemented in the
<code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code> function, and can be used by setting
<code>test = TRUE</code>. For each estimation function, homogeneity
testing is performed using a permutation test on the divergence
statistic, and the corresponding
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-value
is reported.</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">dr</span>, test <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; ulsif(df_numerator = high_school, df_denominator = no_high_school,     parallel = TRUE, nthreads = 10)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Kernel Information:</span></span>
<span><span class="co">#&gt;   Kernel type: Gaussian with L2 norm distances</span></span>
<span><span class="co">#&gt;   Number of kernels: 200</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Optimal sigma: 1.387557</span></span>
<span><span class="co">#&gt; Optimal lambda: 0.6951928</span></span>
<span><span class="co">#&gt; Optimal kernel weights: num [1:201] 0.16916 0.00301 0.05082 -0.00791 0.03594 ...</span></span>
<span><span class="co">#&gt;  </span></span>
<span><span class="co">#&gt; Pearson divergence between P(nu) and P(de): 0.4149</span></span>
<span><span class="co">#&gt; Pr(P(nu)=P(de)) &lt; .001</span></span>
<span><span class="co">#&gt; Bonferroni-corrected for testing with r(x) = P(nu)/P(de) AND r*(x) = P(de)/P(nu).</span></span></code></pre></div>
<p>This permutation test shows that it is highly unlikely that the high
school and no high school samples are drawn from the same distribution.
Intuitively, this is no surprise, as it is quite likely that the two
samples have different marginal distributions for the features.</p>
<p>The permutation test can also be executed in parallel, by setting
<code>parallel = TRUE</code>. Parallel computation is performed using
the <code>pbreplicate</code> function from the <code>pbapply</code>
package. To specify the cluster, users can directly create and supply a
cluster using the <code><a href="https://rdrr.io/r/parallel/makeCluster.html" class="external-link">parallel::makeCluster()</a></code> functionality
(see the documentation in the <code>pbapply</code> and
<code>parallel</code> packages). If no cluster is supplied, a default
cluster is created using the number of available cores minus one.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">dr</span>, test <span class="op">=</span> <span class="cn">TRUE</span>, parallel <span class="op">=</span> <span class="cn">TRUE</span>, cluster <span class="op">=</span> <span class="fl">12</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; ulsif(df_numerator = high_school, df_denominator = no_high_school,     parallel = TRUE, nthreads = 10)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Kernel Information:</span></span>
<span><span class="co">#&gt;   Kernel type: Gaussian with L2 norm distances</span></span>
<span><span class="co">#&gt;   Number of kernels: 200</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Optimal sigma: 1.387557</span></span>
<span><span class="co">#&gt; Optimal lambda: 0.6951928</span></span>
<span><span class="co">#&gt; Optimal kernel weights: num [1:201] 0.16916 0.00301 0.05082 -0.00791 0.03594 ...</span></span>
<span><span class="co">#&gt;  </span></span>
<span><span class="co">#&gt; Pearson divergence between P(nu) and P(de): 0.4149</span></span>
<span><span class="co">#&gt; Pr(P(nu)=P(de)) &lt; .001</span></span>
<span><span class="co">#&gt; Bonferroni-corrected for testing with r(x) = P(nu)/P(de) AND r*(x) = P(de)/P(nu).</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="visualizing-the-density-ratio">Visualizing the density ratio<a class="anchor" aria-label="anchor" href="#visualizing-the-density-ratio"></a>
</h2>
<p>When mere testing is not sufficient, visualizing the density ratio
can provide more tangible information on the differences between the two
distributions. The <code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> function summarizes the model
output by displaying the estimated density ratio values.</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">dr</span><span class="op">)</span></span>
<span><span class="co">#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</span></span></code></pre></div>
<div class="float">
<img src="densityratio-plot-dr-1.png" alt="Density ratio values for the high school and no high school samples."><div class="figcaption">Density ratio values for the high school and no
high school samples.</div>
</div>
<p>The figure shows the distribution of density ratio values for the
numerator and denominator samples (on a log-scale, to improve symmetry).
This allows to evaluate how dissimilar the two distributions are: the
more the density ratio values are separated, the more the two groups
stand apart. That is, if the two distributions are very similar, the
density ratio will be close to zero on the log-scale (one on the
original scale) for all samples, regardless of the group, but more
importantly, the height of the bars will be similar for both groups at
all density ratio values.</p>
<p>Additionally, it is possible to plot the estimated density ratio
values against the features, both univariate and bivariate. This may
help to identify features, or combinations of features, on which the two
groups differ. For example, below we plot the density ratio values
against <code>kid_score</code> and <code>mom_iq</code> in a grid (by
specifying <code>grid = TRUE</code>, the default
<code>grid = FALSE</code> produces a list with separate figures for each
feature).</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/plot_univariate.html">plot_univariate</a></span><span class="op">(</span><span class="va">dr</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"kid_score"</span>, <span class="st">"mom_iq"</span><span class="op">)</span>, grid <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<div class="float">
<img src="densityratio-plot-univariate-1.png" alt="Univariate plots of kid_score and mom_iq against the estimated density ratio values for the two groups of samples."><div class="figcaption">Univariate plots of <code>kid_score</code> and
<code>mom_iq</code> against the estimated density ratio values for the
two groups of samples.</div>
</div>
<p>This figure shows that the density ratio values do not seem to vary
so much with <code>kid_score</code>, but do differ substantially with
<code>mom_iq</code>. That is, for <code>kid_score</code>, the samples of
the two groups mix quite well, although there might be slightly more
<code>no high school</code> samples with a lower <code>kid_score</code>,
resulting in slightly lower density ratio values in this region. For
<code>mom_iq</code> this difference is much more pronounced: lower
values of <code>mom_iq</code> are typically associated with lower
density ratio values, indicating that there are typically more
<code>no high school</code> samples with lower <code>mom_iq</code>
values.</p>
<p>Similar figures can be created for bivariate features, by using the
<code><a href="../reference/plot_bivariate.html">plot_bivariate()</a></code> function, which allows to visualize the
density ratio values against two features at the same time. Now, the
density ratio values are mapped to the color scale.</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/plot_bivariate.html">plot_bivariate</a></span><span class="op">(</span><span class="va">dr</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"kid_score"</span>, <span class="st">"mom_iq"</span>, <span class="st">"mom_age"</span><span class="op">)</span>, grid <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<div class="float">
<img src="densityratio-plot-bivariate-1.png" alt="Bivariate scatter plots for kid_score, mom_iq and mom_age, with density ratio values mapped to the color scale."><div class="figcaption">Bivariate scatter plots for
<code>kid_score</code>, <code>mom_iq</code> and <code>mom_age</code>,
with density ratio values mapped to the color scale.</div>
</div>
<p>This figure shows that density ratio values are especially small for
observations with low values for both <code>mom_iq</code> and
<code>kid_score</code>. The values do not differ so much in the
direction of <code>mom_age</code>.</p>
</div>
<div class="section level2">
<h2 id="predicting-the-density-ratio-for-new-cases">Predicting the density ratio for new cases<a class="anchor" aria-label="anchor" href="#predicting-the-density-ratio-for-new-cases"></a>
</h2>
<p>Under the hood, the plotting functionality makes extensive use of the
<code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code> function, which takes a fitted density ratio
object and a new data set as input, and returns the predicted density
ratio values for these new cases. By default, the <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>
function predicts the density ratio values for the numerator samples,
but this can be changed by providing another data set in the
<code>newdata</code> argument.</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">dr</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; , , 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;           [,1]</span></span>
<span><span class="co">#&gt; [1,] 1.4928033</span></span>
<span><span class="co">#&gt; [2,] 2.2715889</span></span>
<span><span class="co">#&gt; [3,] 2.5855672</span></span>
<span><span class="co">#&gt; [4,] 1.9345356</span></span>
<span><span class="co">#&gt; [5,] 1.9679638</span></span>
<span><span class="co">#&gt; [6,] 0.7939163</span></span></code></pre></div>
<p>By default, the <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code> function uses the optimal
hyperparameter values as determined by cross-validation (if
cross-validation is performed, otherwise it returns the predicted
density ratio values for all parameters). By changing the parameters
<code>sigma</code> (and, if applicable, <code>lamdba</code> or
<code>m</code>), it is also possible to predict the density ratio using
different parameters. These values can alternatively be set to ‚Äúall‚Äù,
which returns the predicted density ratio values for all parameter
values, or a specific value, which returns the predicted density ratio
values for that specific hyperparameter value.</p>
</div>
<div class="section level2">
<h2 id="conclusion">Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"></a>
</h2>
<p>This vignette showed how the <code>densityratio</code> package can be
used to compare samples from two distributions. This allows for two
sample homogeneity testing, and to visualize the differences between the
two distribution.</p>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<p>Huang, J., Smola, A., Gretton, K., Borgwardt, K. M., &amp; Sch√∂lkopf,
B. (2007). Correcting sample selection bias by unlabeled data.
<em>Advances in Neural Information Processing Systems, 19</em>,
601-608.</p>
<p>Izbicki, R., Lee, A., &amp; Schafer, C. (2014). High-dimensional
density ratio estimation with extensions to approximate likelihood
computation. <em>Proceedings of Machine Learning Research, 33</em>,
420-429.</p>
<p>Kanamori, T., Hido, S., &amp; Sugiyama, M. (2009). A least-squares
approach to direct importance estimation. <em>Journal of Machine
Learning Research, 10</em>, 1391-1445.</p>
<p>Sugiyama, M., Nakajima, S., Kashima, H., Von B√ºnau, P., &amp;
Kawanabe, M. (2007). Direct importance estimation with model selection
and its application to covariate shift adaptation. <em>Advances in
Neural Information Processing Systems, 20</em>.</p>
<p>Sugiyama, M., Yamada, M., Von B√ºnau, P., Suzuki, T., Kanamori, T.,
&amp; Kawanabe, M. (2011). Direct density-ratio estimation with
dimensionality reduction via least-squares hetero-distributional
subspace search. <em>Neural Networks, 24(2)</em>, 183-198.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Thom Volker.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.2.</p>
</div>

    </footer>
</div>





  </body>
</html>
