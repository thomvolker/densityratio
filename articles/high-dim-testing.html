<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>High dimensional two-sample testing • densityratio</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png">
<link rel="icon" type="”image/svg+xml”" href="../favicon.svg">
<link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" sizes="any" href="../favicon.ico">
<link rel="manifest" href="../site.webmanifest">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="High dimensional two-sample testing">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">densityratio</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/densityratio.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/covariate-shift.html">Covariate shift adjustment</a></li>
    <li><a class="dropdown-item" href="../articles/high-dim-testing.html">High dimensional two-sample testing</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>High dimensional two-sample testing</h1>
            
      

      <div class="d-none name"><code>high-dim-testing.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>The main use of density ratio estimation is informative distribution
comparison. Informative, because it allows to evaluate how two
distributions differ at every region of the space of the data. Consider
that we have samples from two, potentially different, distributions,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mtext mathvariant="normal">nu</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p_\text{nu}(x)</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mtext mathvariant="normal">de</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p_\text{de}(x)</annotation></semantics></math>.
Then, the density ratio between the two distributions is defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><msub><mi>p</mi><mtext mathvariant="normal">nu</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><msub><mi>p</mi><mtext mathvariant="normal">de</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">
r(x) = \frac{p_\text{nu}(x)}{p_\text{de}(x)},
</annotation></semantics></math> and can take any value between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0</mn><annotation encoding="application/x-tex">0</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>∞</mi><annotation encoding="application/x-tex">\infty</annotation></semantics></math>,
according to whether the numerator or denominator distribution is larger
at location
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>,
which is defined over the multivariate space of the data. Differences
between two distributions can be summarized using divergence measures
(such as the Pearson or Kullback-Leibler divergence), and these
divergences measures can again be used to test the null hypothesis that
two distributions are equal. In this vignette, we show how to use
density ratio estimation and the <code>densityratio</code> package to
perform two-sample testing.</p>
</div>
<div class="section level2">
<h2 id="two-sample-testing-with-density-ratios">Two-sample testing with density ratios<a class="anchor" aria-label="anchor" href="#two-sample-testing-with-density-ratios"></a>
</h2>
<p>Consider that we have samples from two
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>-dimensional
distributions,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mtext mathvariant="normal">nu</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p_\text{nu}(x)</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mtext mathvariant="normal">de</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p_\text{de}(x)</annotation></semantics></math>,
and we want to test the null hypothesis that the samples come from the
same distribution (that is,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mtext mathvariant="normal">nu</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>p</mi><mtext mathvariant="normal">de</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p_\text{nu}(x) = p_\text{de}(x)</annotation></semantics></math>).
If we are interested solely in differences in the means of the
distributions, we could potentially use a (multivariate)
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>-test
(e.g., Hotelling’s
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>-squared),
but this might not be feasible if the variance-covariance matrix is not
invertible. However, if we are interested in more general differences
between distributions, we would have to settle for a non-parametric
test, such as a multivariate extension of the Kolmogorov-Smirnov test.
Alternatively, we can use divergence based tests, which are based on
some divergence measure (see, e.g., Sugiyama et al., 2011), which can
have higher power than the Kolmogorov-Smirnov test (see, e.g., Volker et
al., 2023). The <code>densityratio</code> package implements multiple
divergence based tests (all implemented in <code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code>),
dependent on the estimation method: <code><a href="../reference/ulsif.html">ulsif()</a></code>,
<code><a href="../reference/spectral.html">spectral()</a></code>, <code><a href="../reference/kmm.html">kmm()</a></code>, and <code><a href="../reference/lhss.html">lhss()</a></code> use
the Pearson divergence, <code><a href="../reference/kliep.html">kliep()</a></code> uses the Kullback-Leibler
divergence. In this vignette, we use the test implemented in the
<code><a href="../reference/spectral.html">spectral()</a></code> density ratio estimation method, which is
particularly tailored towards high-dimensional data.</p>
<p>The Pearson divergence is defined as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>P</mi><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>p</mi><mtext mathvariant="normal">nu</mtext></msub><mo>,</mo><msub><mi>p</mi><mtext mathvariant="normal">de</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>∫</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><msub><mi>p</mi><mtext mathvariant="normal">nu</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><msub><mi>p</mi><mtext mathvariant="normal">de</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><msub><mi>p</mi><mtext mathvariant="normal">de</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>x</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>∫</mo><mi>r</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>p</mi><mtext mathvariant="normal">nu</mtext></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>x</mi><mo>−</mo><mo>∫</mo><mi>r</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>p</mi><mtext mathvariant="normal">de</mtext></msub><mi>d</mi><mi>x</mi><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>,</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
PE(p_\text{nu}, p_\text{de}) &amp;= \frac 1 2 \int \left( \frac{p_\text{nu}(x)}{p_\text{de}(x)} - 1 \right)^2 p_\text{de}(x) dx \\
&amp;= \frac 1 2 \int r(x) p_\text{nu}(x) dx - \int r(x) p_\text{de} dx + \frac 1 2,
\end{aligned}
</annotation></semantics></math> and can be interpreted as the expected
squared difference of the density ratio from unity, over the denominator
distribution. If the two distributions are (almost) equal, the squared
deviation will be small, and thus the Pearson divergence will be small.
When there are large differences between the two distributions, the
squared deviation will be large, and thus the Pearson divergence will be
large. Since we do not know the numerator and denominator densities, nor
the density ratio, we have to estimate the Pearson divergence from the
samples. The density ratio can be estimated using the
<code><a href="../reference/spectral.html">spectral()</a></code> method (see the <a href="https://thomvolker.github.io/densityratio/articles/densityratio.html">Get
Started vignette</a> and Izbicki et al., 2014). Subsequently, can
estimate the Pearson divergence empirically, by averaging the density
ratios over the numerator and denominator samples. That is, we estimate
the Pearson divergence as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mrow><mi>P</mi><mi>E</mi></mrow><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>p</mi><mtext mathvariant="normal">nu</mtext></msub><mo>,</mo><msub><mi>p</mi><mtext mathvariant="normal">de</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><msub><mi>n</mi><mtext mathvariant="normal">nu</mtext></msub></mrow></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mtext mathvariant="normal">nu</mtext></msub></munderover><mover><mi>r</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>x</mi><mi>i</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">nu</mtext><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mfrac><mn>1</mn><msub><mi>n</mi><mtext mathvariant="normal">de</mtext></msub></mfrac><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mtext mathvariant="normal">de</mtext></msub></munderover><mover><mi>r</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>x</mi><mi>j</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">de</mtext><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">
\hat{PE}(p_\text{nu}, p_\text{de}) =
\frac{1}{2n_\text{nu}} \sum_{i = 1}^{n_\text{nu}} \hat{r}(x_i^{(\text{nu})}) - \frac{1}{n_\text{de}} \sum_{j = 1}^{n_\text{de}} \hat{r}(x_j^{(\text{de})}) + \frac{1}{2},
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>r</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{r}(x)</annotation></semantics></math>
denotes the estimated density ratio at location
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mtext mathvariant="normal">nu</mtext></msub><annotation encoding="application/x-tex">n_\text{nu}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mtext mathvariant="normal">de</mtext></msub><annotation encoding="application/x-tex">n_\text{de}</annotation></semantics></math>
are the number of samples from the numerator and denominator
distributions, respectively.</p>
<p>Finally, we can compare the estimated Pearson divergence to a
reference distribution. However, to the best of my knowledge, there is
no known reference distribution for the Pearson divergence, and since it
is a non-negative quantity, a normal approximation might not be
feasible. Therefore, we use a permutation test to obtain a null
distribution, as proposed by Sugiyama et al. (2011). That is, we
randomly re-allocate the samples from the numerator and denominator
distributions, estimate the density ratio function, and compute the
Pearson divergence repeatedly, such that we obtain a reference
distribution of Pearson divergences under the null hypothesis.
Subsequently, we evaluate the probability that the obtained Pearson
divergence is larger than the Pearson divergences from the null
distribution, which gives rise to a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-value.
This approach achieves nominal type I error control rates, as shown by
Volker (2025).</p>
</div>
<div class="section level2">
<h2 id="empirical-example">Empirical example<a class="anchor" aria-label="anchor" href="#empirical-example"></a>
</h2>
<p>To illustrate the divergence-based test, we use the
<code>colon</code> dataset (included in the <code>densityratio</code>
package), which contains the expression levels of 2000 genes in 22 colon
tumor tissues, and 40 non-tumor tissues (Alon et al., 1999). Our goal is
to evaluate whether the expression levels of the genes are different for
the two groups (over all genes simultaneously).</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://thomvolker.github.io/densityratio/">densityratio</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">numerator</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/subset.html" class="external-link">subset</a></span><span class="op">(</span><span class="va">colon</span>, <span class="va">class</span> <span class="op">==</span> <span class="st">"tumor"</span>, select <span class="op">=</span> <span class="op">-</span><span class="va">class</span><span class="op">)</span></span>
<span><span class="va">denominator</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/subset.html" class="external-link">subset</a></span><span class="op">(</span><span class="va">colon</span>, <span class="va">class</span> <span class="op">==</span> <span class="st">"normal"</span>, select <span class="op">=</span> <span class="op">-</span><span class="va">class</span><span class="op">)</span></span>
<span></span>
<span><span class="va">dr</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/spectral.html">spectral</a></span><span class="op">(</span><span class="va">numerator</span>, <span class="va">denominator</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">dr</span>, test <span class="op">=</span> <span class="cn">TRUE</span>, parallel <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; spectral(df_numerator = numerator, df_denominator = denominator)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Kernel Information:</span></span>
<span><span class="co">#&gt;   Kernel type: Gaussian with L2 norm distances</span></span>
<span><span class="co">#&gt;   Number of kernels: 40</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Optimal sigma: 110.8282</span></span>
<span><span class="co">#&gt; Optimal subspace: 35</span></span>
<span><span class="co">#&gt; Optimal kernel weights (cv): num [1:35] 1.05247 0.56802 0.00296 0.15847 -0.24053 ...</span></span>
<span><span class="co">#&gt;  </span></span>
<span><span class="co">#&gt; Pearson divergence between P(nu) and P(de): 1.972</span></span>
<span><span class="co">#&gt; Pr(P(nu)=P(de)) &lt; .001</span></span>
<span><span class="co">#&gt; Bonferroni-corrected for testing with r(x) = P(nu)/P(de) AND r*(x) = P(de)/P(nu).</span></span></code></pre></div>
<p>The <code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code> function computes the Pearson divergence,
and performs a permutation test to evaluate the null hypothesis that the
two distributions are equal. In this case, the probability that the
samples come from the same distribution is very small, and thus the gene
expression levels are different between the two groups. Evaluating which
genes are most important for this difference is not straightforward
which such high-dimensional data, and would require alternative methods
(perhaps dimension reduction before conducting density ratio estimation,
or a lasso-type analysis).</p>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<p>Alon, U., Barkai, N., Notterman, D. A., Gish, K., Ybarra, S., Mack,
D., &amp; Levine, A. J. (1999). Broad patterns of gene expression
revealed by clustering of tumor and normal colon tissues probed by
oligonucleotide arrays. Proceedings of the National Academy of Sciences,
96(12), 6745-6750. <a href="https://doi.org/10.1073/pnas.96.12.6745" class="external-link uri">https://doi.org/10.1073/pnas.96.12.6745</a></p>
<p>Izbicki, R., Lee, A., &amp; Schafer, C. (2014). High-dimensional
density ratio estimation with extensions to approximate likelihood
computation. <em>Proceedings of Machine Learning Research, 33</em>,
420-429. <a href="https://proceedings.mlr.press/v33/izbicki14.html" class="external-link uri">https://proceedings.mlr.press/v33/izbicki14.html</a></p>
<p>Sugiyama, M., Suzuki, T., Itoh, Y., Kanamori, T., &amp; Kimura, M.
(2011). Least-squares two-sample test. <em>Neural Networks, 24</em>,
735-751. <a href="http://dx.doi.org/10.1016/j.neunet.2011.04.003" class="external-link uri">http://dx.doi.org/10.1016/j.neunet.2011.04.003</a></p>
<p>Volker, T. B. (2025). Divergence-based testing using density ratio
estimation techniques. <a href="https://gist.github.com/thomvolker/58197e535ec458752bccbb5b611046ce" class="external-link uri">https://gist.github.com/thomvolker/58197e535ec458752bccbb5b611046ce</a></p>
<p>Volker, T. B., de Wolf, P.-P., &amp; Van Kesteren, E.-J. (2023).
Assessing the utility of synthetic data: A density ratio perspective.
UNECE Expert Meeting on Statistical Data Confidentiality. <a href="https://doi.org/10.5281/zenodo.8315054" class="external-link uri">https://doi.org/10.5281/zenodo.8315054</a></p>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Thom Volker.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.2.</p>
</div>

    </footer>
</div>





  </body>
</html>
